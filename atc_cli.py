#!/usr/bin/env python3
"""
ATC Hallucination Project - Unified CLI

A comprehensive command-line interface for the ATC Hallucination project that provides:
- Scenario generation with custom parameters
- Model training with multiple algorithms
- Distribution shift testing (unison and intrashift)
- Intershift matrix analysis (cross-scenario robustness testing)
- Hallucination detection and analysis  
- Visualization and reporting
- Model evaluation and checkpoint management

Usage examples:
    # Generate scenarios
    python atc_cli.py generate-scenarios --all
    python atc_cli.py generate-scenarios --scenario head_on --params "approach_nm=20"
    
    # Train models
    python atc_cli.py train --scenario head_on --algo PPO --timesteps 100000
    python atc_cli.py train --scenario all --timesteps 50000 --checkpoint-every 10000
    python atc_cli.py train --algo PPO --timesteps 2000000 --scenario canonical_crossing --log-trajectories
    python atc_cli.py train --env-type generic --algo PPO --timesteps 1000000  # Dynamic conflict generation
    python atc_cli.py train --env-type generic --scenario generic --algo SAC --timesteps 500000 --gpu
    
    # Run shift testing
    python atc_cli.py test-shifts --checkpoint latest --scenario parallel --episodes 5
    python atc_cli.py test-shifts --intrashift --episodes 3 --viz
    
    # Intershift matrix analysis (cross-scenario robustness testing)
    python atc_cli.py intershift-matrix --episodes 5 --use-gpu
    python atc_cli.py intershift-matrix --extensive --models-index models_config.json
    
    # Analyze and visualize
    python atc_cli.py analyze --results-dir results_PPO_head_on_20250923_190203
    python atc_cli.py visualize --trajectory traj_ep_0001.csv
    
    # Full pipeline
    python atc_cli.py full-pipeline --scenario head_on --train-timesteps 50000 --test-episodes 3
    python atc_cli.py full-pipeline --env-type generic --scenario generic --train-timesteps 100000
"""

import os
import sys
import argparse
import json
import time
import logging
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional, Union

# Add project root to Python path
project_root = Path(__file__).parent.absolute()
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger("ATC_CLI")

class ATCController:
    """Main controller for the ATC Hallucination project CLI."""
    
    def __init__(self, repo_root: str):
        self.repo_root = Path(repo_root)
        self.scenarios_dir = self.repo_root / "scenarios"
        self.results_dir = self.repo_root / "results"
        self.models_dir = self.repo_root / "models"
        
        # Ensure directories exist
        for dir_path in [self.scenarios_dir, self.results_dir, self.models_dir]:
            dir_path.mkdir(exist_ok=True)
    
    def generate_scenarios(self, scenario_types: Optional[List[str]] = None, scenario_params: Optional[Dict[str, Any]] = None) -> List[str]:
        """Generate scenario files using the scenario generator."""
        try:
            from src.scenarios.scenario_generator import (
                make_head_on, make_t_formation, make_parallel, make_converging, make_canonical_crossing
            )
            
            scenario_funcs = {
                "head_on": make_head_on,
                "t_formation": make_t_formation,
                "parallel": make_parallel,
                "converging": make_converging,
                "canonical_crossing": make_canonical_crossing
            }
            
            if scenario_types is None or "all" in scenario_types:
                scenario_types = list(scenario_funcs.keys())
            
            generated_paths = []
            
            for scenario_name in scenario_types:
                if scenario_name not in scenario_funcs:
                    logger.warning(f"Unknown scenario type: {scenario_name}")
                    continue
                
                logger.info(f"Generating scenario: {scenario_name}")
                
                # Apply custom parameters if provided
                params = scenario_params.get(scenario_name, {}) if scenario_params else {}
                
                try:
                    path = scenario_funcs[scenario_name](**params)
                    generated_paths.append(path)
                    logger.info(f"Generated: {path}")
                except Exception as e:
                    logger.error(f"Failed to generate {scenario_name}: {e}")
            
            return generated_paths
            
        except ImportError as e:
            logger.error(f"Failed to import scenario generator: {e}")
            return []
    
    def train_model(self, scenario_names: Union[str, List[str]], algo: str = "PPO", timesteps: int = 100000, 
                   checkpoint_every: int = 10000, log_trajectories: bool = False, 
                   env_type: str = "frozen", **kwargs) -> Optional[str]:
        """Train a model on the specified scenario(s) and algorithm(s).
        
        Args:
            scenario_names: Scenario name(s) or "generic" for dynamic conflict generation
            algo: Algorithm to use (PPO, SAC, IMPALA, CQL, APPO)
            timesteps: Number of training timesteps
            checkpoint_every: Checkpoint save frequency
            log_trajectories: Enable detailed trajectory logging
            env_type: Environment type ("frozen" for scenarios, "generic" for dynamic conflicts)
            **kwargs: Additional training parameters
        """
        try:
            from src.training.train_frozen_scenario import train_frozen
            
            # Handle scenario names input
            if isinstance(scenario_names, str):
                scenario_names = [scenario_names]
            
            # Handle generic training
            if env_type == "generic":
                # For generic training, use "generic" as scenario name
                scenarios = ["generic"]
                logger.info("Using generic environment for dynamic conflict generation")
            else:
                # Handle 'all' scenarios for frozen training
                if any(s.lower() == "all" for s in scenario_names):
                    scenarios = self.list_scenarios()
                    if not scenarios:
                        logger.warning("No scenarios found, generating all...")
                        self.generate_scenarios(["all"])
                        scenarios = self.list_scenarios()
                else:
                    scenarios = scenario_names
            
            # Handle 'all' algorithms
            if algo.lower() == "all":
                algorithms = ["PPO", "SAC", "IMPALA", "CQL", "APPO"]
            else:
                algorithms = [algo]
            
            final_checkpoints = []
            
            # Train each combination
            for current_algo in algorithms:
                for current_scenario in scenarios:
                    logger.info(f"Training {current_algo} on scenario '{current_scenario}' for {timesteps:,} timesteps")
                    
                    # Ensure scenario exists (for frozen environments only)
                    if env_type == "frozen":
                        scenario_path = self.scenarios_dir / f"{current_scenario}.json"
                        if not scenario_path.exists():
                            logger.warning(f"Scenario {current_scenario} not found, generating...")
                            self.generate_scenarios([current_scenario])
                    
                    checkpoint_path = train_frozen(
                        repo_root=str(self.repo_root),
                        algo=current_algo,
                        scenario_name=current_scenario,
                        timesteps_total=timesteps,
                        checkpoint_every=checkpoint_every,
                        log_trajectories=log_trajectories,
                        env_type=env_type,
                        **kwargs
                    )
                    
                    if checkpoint_path:
                        final_checkpoints.append(checkpoint_path)
                        logger.info(f"Training completed for {current_algo} on {current_scenario}. Checkpoint: {checkpoint_path}")
                    else:
                        logger.error(f"Training failed for {current_algo} on {current_scenario}")
            
            if final_checkpoints:
                logger.info(f"All training completed. {len(final_checkpoints)} models trained.")
                return final_checkpoints[-1]  # Return last checkpoint for compatibility
            else:
                logger.error("All training attempts failed.")
                return None
            
        except Exception as e:
            logger.error(f"Training failed: {e}")
            return None
    
    def run_shift_testing(self, checkpoint_path: Optional[str] = None, scenario_name: str = "parallel", 
                         episodes: int = 3, targeted: bool = True, generate_viz: bool = False,
                         algo: str = "PPO", seeds: Optional[List[int]] = None, 
                         outdir: Optional[str] = None) -> Optional[str]:
        """Run distribution shift testing (unison or targeted)."""
        try:
            # Clean up any existing Ray processes first
            try:
                import ray
                if ray.is_initialized():
                    logger.info("Shutting down existing Ray session...")
                    ray.shutdown()
            except Exception as e:
                logger.warning(f"Ray cleanup warning: {e}")
                
            if checkpoint_path is None:
                checkpoint_path = self._auto_detect_checkpoint()
                if checkpoint_path is None:
                    logger.error("No checkpoint found. Please train a model first.")
                    return None
            
            if targeted:
                from src.testing.intrashift_tester import run_intrashift_grid
                from ray.rllib.algorithms.ppo import PPO
                from ray.rllib.algorithms.sac import SAC
                
                algo_class = SAC if algo.upper() == "SAC" else PPO
                
                logger.info(f"Running targeted shift testing with {episodes} episodes per shift")
                
                result_path = run_targeted_shift_grid(
                    repo_root=str(self.repo_root),
                    algo_class=algo_class,
                    checkpoint_path=checkpoint_path,
                    scenario_name=scenario_name,
                    episodes_per_shift=episodes,
                    seeds=seeds,
                    generate_viz=generate_viz,
                    outdir=outdir
                )
                
                logger.info(f"Intrashift testing completed: {result_path}")
                return result_path
            else:
                # Implement basic unison shift testing
                logger.info(f"Running unison shift testing with {episodes} episodes per shift")
                return self._run_unison_shift_testing(checkpoint_path, scenario_name, episodes, generate_viz, algo)
                
        except Exception as e:
            logger.error(f"Shift testing failed: {e}")
            return None
    
    def _run_unison_shift_testing(self, checkpoint_path: str, scenario_name: str, 
                                 episodes: int, generate_viz: bool, algo: str) -> Optional[str]:
        """Run unison shift testing (simplified implementation)."""
        try:
            # For now, fall back to intrashift testing with a warning
            logger.warning("Unison shift testing not fully implemented. Running intrashift testing instead.")
            
            from src.testing.intrashift_tester import run_intrashift_grid
            from ray.rllib.algorithms.ppo import PPO
            from ray.rllib.algorithms.sac import SAC
            
            algo_class = SAC if algo.upper() == "SAC" else PPO
            
            result_path = run_intrashift_grid(
                repo_root=str(self.repo_root),
                algo_class=algo_class,
                checkpoint_path=checkpoint_path,
                scenario_name=scenario_name,
                episodes_per_shift=episodes,
                generate_viz=generate_viz
            )
            
            return result_path
            
        except Exception as e:
            logger.error(f"Unison shift testing failed: {e}")
            return None
    
    def analyze_results(self, results_dir: Optional[str] = None, hallucination_analysis: bool = True) -> Dict[str, Any]:
        """Analyze training or testing results."""
        try:
            results_path: Path
            if results_dir is None:
                # Find latest results directory
                results_dirs = [d for d in self.results_dir.iterdir() if d.is_dir()]
                if not results_dirs:
                    logger.error("No results directories found")
                    return {}
                results_path = max(results_dirs, key=lambda x: x.stat().st_mtime)
            else:
                results_path = Path(results_dir)
            
            logger.info(f"Analyzing results in: {results_path}")
            
            analysis_results = {
                "results_dir": str(results_path),
                "timestamp": datetime.now().isoformat()
            }
            
            # Basic trajectory analysis
            traj_files = list(results_path.glob("traj_ep_*.csv")) + list(results_path.glob("**/traj_ep_*.csv")) + list(results_path.glob("traj_*.csv")) + list(results_path.glob("**/traj_*.csv"))
            if traj_files:
                logger.info(f"Found {len(traj_files)} trajectory files")
                analysis_results["trajectory_files"] = len(traj_files)
                
                # Analyze latest trajectory
                latest_traj = max(traj_files, key=lambda x: x.stat().st_mtime)
                analysis_results.update(self._analyze_trajectory(latest_traj))
            
            # Hallucination analysis if enabled
            if hallucination_analysis and traj_files:
                analysis_results["hallucination"] = self._analyze_hallucinations(traj_files[0])
            
            # Training progress analysis
            progress_file = results_path / "training_progress.csv"
            if progress_file.exists():
                analysis_results["training"] = self._analyze_training_progress(progress_file)
            
            return analysis_results
            
        except Exception as e:
            logger.error(f"Analysis failed: {e}")
            return {}
    
    def visualize_data(self, trajectory_path: Optional[str] = None, results_dir: Optional[str] = None, 
                      output_dir: Optional[str] = None) -> List[str]:
        """Generate visualizations for trajectories and results."""
        try:
            output_path: Path
            if output_dir is None:
                output_path = self.repo_root / "vis_temp"
            else:
                output_path = Path(output_dir)
            
            output_path.mkdir(exist_ok=True)
            
            generated_files = []
            
            if trajectory_path:
                logger.info(f"Visualizing trajectory: {trajectory_path}")
                # TODO: Implement trajectory visualization
                # from src.analysis.viz_hooks import make_episode_visuals
                # generated_files.extend(make_episode_visuals(...))
            
            if results_dir:
                logger.info(f"Visualizing results: {results_dir}")
                # TODO: Implement results visualization
                # from src.analysis.viz_hooks import make_run_visuals
                # generated_files.extend(make_run_visuals(...))
            
            logger.info(f"Generated {len(generated_files)} visualization files")
            return generated_files
            
        except Exception as e:
            logger.error(f"Visualization failed: {e}")
            return []
    
    def run_intershift_matrix(self, models_index: Optional[str] = None, models_dir: str = "models",
                             episodes: int = 5, outdir: str = "results_intershift",
                             use_gpu: bool = False, extensive: bool = False,
                             scenarios_dir: str = "scenarios") -> Optional[str]:
        """Run intershift matrix analysis comparing trained models against scenario shifts."""
        try:
            # Import the intershift_matrix module
            from src.testing.intershift_matrix import main as run_matrix_analysis
            import sys
            
            # Prepare arguments for the matrix analysis
            matrix_args = [
                "--models-dir", str(self.repo_root / models_dir),
                "--scenarios-dir", str(self.repo_root / scenarios_dir),
                "--episodes", str(episodes),
                "--outdir", str(self.repo_root / outdir)
            ]
            
            if models_index:
                matrix_args.extend(["--models-index", models_index])
            
            if use_gpu:
                matrix_args.append("--use-gpu")
            
            if extensive:
                matrix_args.append("--extensive")
            
            # Save current sys.argv and replace with our arguments
            original_argv = sys.argv
            sys.argv = ["intershift_matrix.py"] + matrix_args
            
            try:
                logger.info("üî¨ Running intershift matrix analysis...")
                logger.info(f"Arguments: {' '.join(matrix_args)}")
                
                # Run the matrix analysis
                run_matrix_analysis()
                
                result_path = str(self.repo_root / outdir / "intershift_summary.csv")
                logger.info(f"‚úÖ Intershift matrix analysis completed: {result_path}")
                return result_path
                
            finally:
                # Restore original sys.argv
                sys.argv = original_argv
                
        except Exception as e:
            logger.error(f"Intershift matrix analysis failed: {e}")
            return None

    def full_pipeline(self, scenario_name: str, algo: str = "PPO", 
                     train_timesteps: int = 50000, test_episodes: int = 3,
                     targeted_shifts: bool = True, generate_viz: bool = True,
                     log_trajectories: bool = False, env_type: str = "frozen") -> Dict[str, Any]:
        """Run the complete pipeline: scenario generation ‚Üí training ‚Üí testing ‚Üí analysis.
        
        Args:
            scenario_name: Scenario name or "generic" for dynamic conflicts
            algo: Algorithm to use
            train_timesteps: Training timesteps
            test_episodes: Testing episodes
            targeted_shifts: Use targeted vs unison shifts
            generate_viz: Generate visualizations
            log_trajectories: Enable trajectory logging
            env_type: Environment type ("frozen" or "generic")
        """
        logger.info("üöÄ Starting full pipeline execution")
        
        pipeline_results = {
            "start_time": datetime.now().isoformat(),
            "scenario": scenario_name,
            "algorithm": algo,
            "steps": {}
        }
        
        try:
            # Step 1: Generate scenario (skip for generic training)
            if env_type == "frozen":
                logger.info("üìù Step 1: Generating scenarios")
                scenarios = self.generate_scenarios([scenario_name])
                pipeline_results["steps"]["scenario_generation"] = {
                    "success": len(scenarios) > 0,
                    "generated": scenarios
                }
                
                if not scenarios:
                    raise Exception("Scenario generation failed")
            else:
                logger.info("üìù Step 1: Using generic environment (scenario generation skipped)")
                pipeline_results["steps"]["scenario_generation"] = {
                    "success": True,
                    "generated": ["generic"]
                }
            
            # Step 2: Train model
            logger.info("üéØ Step 2: Training model")
            checkpoint = self.train_model(
                scenario_names=[scenario_name],
                algo=algo,
                timesteps=train_timesteps,
                checkpoint_every=max(1000, train_timesteps // 10),
                log_trajectories=log_trajectories,
                env_type=env_type
            )
            pipeline_results["steps"]["training"] = {
                "success": checkpoint is not None,
                "checkpoint": checkpoint
            }
            
            if not checkpoint:
                raise Exception("Training failed")
            
            # Step 3: Shift testing
            logger.info("üß™ Step 3: Running shift testing")
            test_results = self.run_shift_testing(
                checkpoint_path=checkpoint,
                scenario_name=scenario_name,
                episodes=test_episodes,
                targeted=targeted_shifts,
                generate_viz=generate_viz,
                algo=algo
            )
            pipeline_results["steps"]["shift_testing"] = {
                "success": test_results is not None,
                "results": test_results
            }
            
            # Step 4: Analysis
            logger.info("üìä Step 4: Analyzing results")
            analysis = self.analyze_results()
            pipeline_results["steps"]["analysis"] = {
                "success": bool(analysis),
                "results": analysis
            }
            
            # Step 5: Visualization (optional)
            if generate_viz:
                logger.info("üìà Step 5: Generating visualizations")
                viz_files = self.visualize_data()
                pipeline_results["steps"]["visualization"] = {
                    "success": len(viz_files) > 0,
                    "files": viz_files
                }
            
            pipeline_results["success"] = True
            pipeline_results["end_time"] = datetime.now().isoformat()
            
            logger.info("‚úÖ Full pipeline completed successfully")
            return pipeline_results
            
        except Exception as e:
            logger.error(f"‚ùå Pipeline failed: {e}")
            pipeline_results["success"] = False
            pipeline_results["error"] = str(e)
            pipeline_results["end_time"] = datetime.now().isoformat()
            return pipeline_results
    
    def list_scenarios(self) -> List[str]:
        """List available scenario files."""
        scenario_files = list(self.scenarios_dir.glob("*.json"))
        return [f.stem for f in scenario_files]
    
    def list_checkpoints(self) -> List[Dict[str, Any]]:
        """List available model checkpoints."""
        checkpoints = []
        
        # Check models directory
        for model_dir in self.models_dir.iterdir():
            if model_dir.is_dir():
                checkpoints.append({
                    "path": str(model_dir),
                    "name": model_dir.name,
                    "modified": datetime.fromtimestamp(model_dir.stat().st_mtime).isoformat()
                })
        
        # Check results directories for model subdirectories
        for results_dir in self.results_dir.iterdir():
            if results_dir.is_dir():
                models_subdir = results_dir / "models"
                if models_subdir.exists():
                    for model_dir in models_subdir.iterdir():
                        if model_dir.is_dir():
                            checkpoints.append({
                                "path": str(model_dir),
                                "name": f"{results_dir.name}/{model_dir.name}",
                                "modified": datetime.fromtimestamp(model_dir.stat().st_mtime).isoformat()
                            })
        
        return sorted(checkpoints, key=lambda x: x["modified"], reverse=True)
    
    def _auto_detect_checkpoint(self) -> Optional[str]:
        """Auto-detect the most recent checkpoint."""
        checkpoints = self.list_checkpoints()
        return checkpoints[0]["path"] if checkpoints else None
    
    def _analyze_trajectory(self, traj_path: Path) -> Dict[str, Any]:
        """Analyze a single trajectory file."""
        try:
            import pandas as pd
            
            df = pd.read_csv(traj_path)
            
            if df.empty:
                return {"error": "Empty trajectory file"}
            
            analysis = {
                "file": str(traj_path),
                "episodes": df["episode_id"].nunique() if "episode_id" in df.columns else 1,
                "steps": len(df),
                "agents": df["agent_id"].nunique() if "agent_id" in df.columns else 0,
            }
            
            # Safety metrics
            if "conflict_flag" in df.columns:
                analysis["conflicts"] = int(df["conflict_flag"].sum())
                analysis["conflict_rate"] = float(df["conflict_flag"].mean())
            
            if "min_separation_nm" in df.columns:
                analysis["min_separation_nm"] = float(df["min_separation_nm"].min())
                analysis["avg_separation_nm"] = float(df["min_separation_nm"].mean())
            
            if "waypoint_reached" in df.columns:
                analysis["waypoint_completion_rate"] = float(df["waypoint_reached"].mean())
            
            return analysis
            
        except Exception as e:
            return {"error": str(e)}
    
    def _analyze_hallucinations(self, traj_path: Path) -> Dict[str, Any]:
        """Run hallucination analysis on trajectory data."""
        try:
            from src.analysis.hallucination_detector_enhanced import HallucinationDetector
            import pandas as pd
            import numpy as np
            
            df = pd.read_csv(traj_path)
            
            # Convert to trajectory format expected by detector
            positions = []
            actions = []
            agents_data = {}
            
            # Get unique agents
            agent_ids = df["agent_id"].unique()
            for aid in agent_ids:
                agents_data[aid] = {"headings": [], "speeds": []}
            
            # Process by step
            for step in sorted(df["step_idx"].unique()):
                step_data = df[df["step_idx"] == step]
                
                pos_dict = {}
                action_dict = {}
                
                for _, row in step_data.iterrows():
                    aid = row["agent_id"]
                    pos_dict[aid] = (row["lat_deg"], row["lon_deg"])
                    action_dict[aid] = np.array([row["action_hdg_delta_deg"], row["action_spd_delta_kt"]])
                    
                    agents_data[aid]["headings"].append(row["hdg_deg"])
                    agents_data[aid]["speeds"].append(row["tas_kt"])
                
                positions.append(pos_dict)
                actions.append(action_dict)
            
            trajectory = {
                "positions": positions,
                "actions": actions,
                "agents": agents_data,
                "timestamps": list(range(len(positions)))
            }
            
            # Run hallucination detection
            detector = HallucinationDetector()
            results = detector.compute(trajectory, sep_nm=5.0, return_series=True)
            
            # Calculate derived metrics from confusion matrix
            tp = results["tp"]
            fp = results["fp"]
            fn = results["fn"]
            tn = results["tn"]
            
            precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
            recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0
            f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0
            accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0.0
            
            return {
                "true_positives": int(tp),
                "false_positives": int(fp),
                "false_negatives": int(fn),
                "true_negatives": int(tn),
                "precision": float(precision),
                "recall": float(recall),
                "f1_score": float(f1_score),
                "accuracy": float(accuracy),
                "resolution_efficiency": float(results.get("resolution_efficiency", 0.0)),
                "unwanted_interventions": int(results.get("unwanted_interventions", 0))
            }
            
        except Exception as e:
            return {"error": str(e)}
    
    def _analyze_training_progress(self, progress_path: Path) -> Dict[str, Any]:
        """Analyze training progress from CSV."""
        try:
            import pandas as pd
            
            df = pd.read_csv(progress_path)
            
            if df.empty:
                return {"error": "Empty progress file"}
            
            return {
                "total_iterations": len(df),
                "final_reward": float(df["reward_mean"].iloc[-1]) if "reward_mean" in df.columns else None,
                "best_reward": float(df["reward_mean"].max()) if "reward_mean" in df.columns else None,
                "total_steps": int(df["steps_sampled"].iloc[-1]) if "steps_sampled" in df.columns else None,
                "zero_conflict_streak": int(df["zero_conflict_streak"].iloc[-1]) if "zero_conflict_streak" in df.columns else None
            }
            
        except Exception as e:
            return {"error": str(e)}


def parse_scenario_params(params_str: str) -> Dict[str, Any]:
    """Parse scenario parameters from string format 'param1=value1,param2=value2'."""
    if not params_str:
        return {}
    
    params = {}
    for param in params_str.split(','):
        if '=' not in param:
            continue
        key, value = param.split('=', 1)
        key = key.strip()
        value = value.strip()
        
        # Try to convert to appropriate type
        if value.lower() in ('true', 'false'):
            params[key] = value.lower() == 'true'
        elif value.replace('.', '').replace('-', '').isdigit():
            params[key] = float(value) if '.' in value else int(value)
        else:
            params[key] = value
    
    return params


def main():
    parser = argparse.ArgumentParser(
        description="ATC Hallucination Project - Unified CLI",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=__doc__
    )
    
    # Global options
    parser.add_argument("--repo-root", type=str, default=str(project_root),
                       help="Project repository root directory")
    parser.add_argument("--verbose", "-v", action="store_true",
                       help="Enable verbose logging")
    
    subparsers = parser.add_subparsers(dest="command", help="Available commands")
    
    # Scenario generation
    gen_parser = subparsers.add_parser("generate-scenarios", help="Generate scenario files")
    gen_parser.add_argument("--scenario", "-s", nargs="+",
                           help="Scenario type(s) to generate (space-separated list)")
    gen_parser.add_argument("--all", action="store_true",
                           help="Generate all scenario types")
    gen_parser.add_argument("--params", type=str,
                           help="Scenario parameters in format 'param1=value1,param2=value2'")
    
    # Training
    train_parser = subparsers.add_parser("train", help="Train models")
    train_parser.add_argument("--scenario", "-s", nargs="+",
                             help="Scenario(s) to train on (space-separated list, or use 'all' for all scenarios)")
    train_parser.add_argument("--algo", "-a", type=str, default="PPO",
                             help="Algorithm to use (PPO, SAC, IMPALA, CQL, APPO, or 'all' for all algorithms)")
    train_parser.add_argument("--timesteps", "-t", type=int, default=100000,
                             help="Number of training timesteps")
    train_parser.add_argument("--checkpoint-every", "-c", type=int,
                             help="Checkpoint frequency (default: timesteps/10)")
    train_parser.add_argument("--gpu", "--use-gpu", action="store_true",
                             help="Enable GPU training (auto-detect if available)")
    train_parser.add_argument("--no-gpu", action="store_true",
                             help="Force CPU-only training")
    train_parser.add_argument("--log-trajectories", action="store_true",
                             help="Enable detailed trajectory logging (default: False for faster training)")
    train_parser.add_argument("--env-type", choices=["frozen", "generic"], default="frozen",
                             help="Environment type: 'frozen' for scenario-based, 'generic' for dynamic conflicts")
    
    # Shift testing
    test_parser = subparsers.add_parser("test-shifts", help="Run distribution shift testing")
    test_parser.add_argument("--checkpoint", type=str,
                            help="Checkpoint path (default: auto-detect latest)")
    test_parser.add_argument("--scenario", "-s", type=str, default="parallel",
                            help="Scenario for testing")
    test_parser.add_argument("--episodes", "-e", type=int, default=3,
                            help="Episodes per shift configuration")
    test_parser.add_argument("--intrashift", action="store_true", default=True,
                            help="Use intrashift testing (vs unison). Default: True")
    test_parser.add_argument("--algo", "-a", choices=["PPO", "SAC"], default="PPO",
                            help="Algorithm type")
    test_parser.add_argument("--viz", action="store_true",
                            help="Generate visualizations")
    test_parser.add_argument("--seeds", type=str,
                            help="Comma-separated list of seeds for reproducible episodes (e.g., 42,123,456)")
    test_parser.add_argument("--outdir", type=str,
                            help="Custom output directory name (default: timestamped)")
    
    # Analysis
    analyze_parser = subparsers.add_parser("analyze", help="Analyze results")
    analyze_parser.add_argument("--results-dir", type=str,
                               help="Results directory (default: latest)")
    analyze_parser.add_argument("--no-hallucination", action="store_true",
                               help="Skip hallucination analysis")
    
    # Visualization
    viz_parser = subparsers.add_parser("visualize", help="Generate visualizations")
    viz_parser.add_argument("--trajectory", type=str,
                           help="Trajectory file to visualize")
    viz_parser.add_argument("--results-dir", type=str,
                           help="Results directory to visualize")
    viz_parser.add_argument("--output-dir", type=str,
                           help="Output directory for visualizations")
    
    # Full pipeline
    pipeline_parser = subparsers.add_parser("full-pipeline", help="Run complete pipeline")
    pipeline_parser.add_argument("--scenario", "-s", type=str, default="head_on",
                                 help="Scenario for pipeline")
    pipeline_parser.add_argument("--algo", "-a", choices=["PPO", "SAC"], default="PPO",
                                 help="Algorithm to use")
    pipeline_parser.add_argument("--train-timesteps", "-t", type=int, default=50000,
                                 help="Training timesteps")
    pipeline_parser.add_argument("--test-episodes", "-e", type=int, default=3,
                                 help="Testing episodes")
    pipeline_parser.add_argument("--no-intrashift", action="store_true",
                                 help="Use unison shifts instead of intrashift")
    pipeline_parser.add_argument("--no-viz", action="store_true",
                                 help="Skip visualization generation")
    pipeline_parser.add_argument("--log-trajectories", action="store_true",
                                 help="Enable detailed trajectory logging during training")
    pipeline_parser.add_argument("--env-type", choices=["frozen", "generic"], default="frozen",
                                 help="Environment type: 'frozen' for scenario-based, 'generic' for dynamic conflicts")
    
    # Intershift Matrix testing
    matrix_parser = subparsers.add_parser("intershift-matrix", 
                                         help="Test trained models against scenario shifts for robustness analysis")
    matrix_parser.add_argument("--models-index", type=str,
                              help="JSON file with model mappings: {'models': {'alias': 'path', ...}, 'baselines': {...}}")
    matrix_parser.add_argument("--models-dir", type=str, default="models",
                              help="Directory containing model checkpoints (default: models)")
    matrix_parser.add_argument("--episodes", "-e", type=int, default=5,
                              help="Episodes per scenario test (default: 5)")
    matrix_parser.add_argument("--outdir", type=str, default="results_intershift",
                              help="Output directory for results (default: results_intershift)")
    matrix_parser.add_argument("--use-gpu", action="store_true",
                              help="Enable GPU acceleration for faster testing")
    matrix_parser.add_argument("--extensive", action="store_true",
                              help="Run extensive testing with 10+ episodes and enhanced analysis")
    matrix_parser.add_argument("--scenarios-dir", type=str, default="scenarios",
                              help="Directory containing scenario files (default: scenarios)")
    
    # List commands
    list_parser = subparsers.add_parser("list", help="List available resources")
    list_parser.add_argument("resource", choices=["scenarios", "checkpoints", "results"],
                            help="Resource type to list")
    
    args = parser.parse_args()
    
    # Configure logging
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    # Initialize controller
    controller = ATCController(args.repo_root)
    
    if not args.command:
        parser.print_help()
        return
    
    try:
        if args.command == "generate-scenarios":
            scenario_types = args.scenario or []
            if args.all:
                scenario_types = ["all"]
            
            scenario_params = {}
            if args.params:
                parsed_params = parse_scenario_params(args.params)
                # Apply same params to all scenarios for now
                for scenario in ["head_on", "t_formation", "parallel", "converging"]:
                    scenario_params[scenario] = parsed_params
            
            paths = controller.generate_scenarios(scenario_types, scenario_params)
            print(f"Generated {len(paths)} scenarios:")
            for path in paths:
                print(f"  {path}")
        
        elif args.command == "train":
            checkpoint_every = args.checkpoint_every or max(1000, args.timesteps // 10)
            
            # Determine GPU usage
            use_gpu = None  # Auto-detect
            if args.gpu:
                use_gpu = True
            elif args.no_gpu:
                use_gpu = False
            
            # Handle scenarios - default to head_on if none specified
            scenarios = args.scenario or ["head_on"]
            
            checkpoint = controller.train_model(
                scenario_names=scenarios,
                algo=args.algo,
                timesteps=args.timesteps,
                checkpoint_every=checkpoint_every,
                use_gpu=use_gpu,
                log_trajectories=args.log_trajectories,
                env_type=args.env_type
            )
            if checkpoint:
                print(f"Training completed. Checkpoint: {checkpoint}")
            else:
                print("Training failed.")
                sys.exit(1)
        
        elif args.command == "test-shifts":
            # Parse seeds if provided
            seeds = None
            if args.seeds:
                try:
                    seeds = [int(s.strip()) for s in args.seeds.split(',')]
                except ValueError:
                    print(f"Error: Invalid seeds format: {args.seeds}")
                    print("Use comma-separated integers, e.g., --seeds 42,123,456")
                    sys.exit(1)
            
            result = controller.run_shift_testing(
                checkpoint_path=args.checkpoint,
                scenario_name=args.scenario,
                episodes=args.episodes,
                targeted=args.intrashift,
                generate_viz=args.viz,
                algo=args.algo,
                seeds=seeds,
                outdir=args.outdir
            )
            if result:
                print(f"Shift testing completed: {result}")
            else:
                print("Shift testing failed.")
                sys.exit(1)
        
        elif args.command == "analyze":
            results = controller.analyze_results(
                results_dir=args.results_dir,
                hallucination_analysis=not args.no_hallucination
            )
            if results:
                print("Analysis Results:")
                print(json.dumps(results, indent=2))
            else:
                print("Analysis failed.")
                sys.exit(1)
        
        elif args.command == "visualize":
            files = controller.visualize_data(
                trajectory_path=args.trajectory,
                results_dir=args.results_dir,
                output_dir=args.output_dir
            )
            print(f"Generated {len(files)} visualization files:")
            for file in files:
                print(f"  {file}")
        
        elif args.command == "intershift-matrix":
            result = controller.run_intershift_matrix(
                models_index=args.models_index,
                models_dir=args.models_dir,
                episodes=args.episodes,
                outdir=args.outdir,
                use_gpu=args.use_gpu,
                extensive=args.extensive,
                scenarios_dir=args.scenarios_dir
            )
            if result:
                print(f"Intershift matrix analysis completed: {result}")
                print("\nüìä Generated comprehensive robustness analysis comparing trained models against scenario shifts.")
                print("üìÅ Check the output directory for:")
                print("  ‚Ä¢ intershift_summary.csv (Performance metrics)")
                print("  ‚Ä¢ summary_*.png (Visualization plots)")
                print("  ‚Ä¢ Interactive trajectory visualizations")
            else:
                print("Intershift matrix analysis failed.")
                sys.exit(1)
        
        elif args.command == "full-pipeline":
            results = controller.full_pipeline(
                scenario_name=args.scenario,
                algo=args.algo,
                train_timesteps=args.train_timesteps,
                test_episodes=args.test_episodes,
                targeted_shifts=not args.no_intrashift,
                generate_viz=not args.no_viz,
                log_trajectories=args.log_trajectories,
                env_type=args.env_type
            )
            
            print("Pipeline Results:")
            print(json.dumps(results, indent=2))
            
            if not results.get("success", False):
                sys.exit(1)
        
        elif args.command == "list":
            if args.resource == "scenarios":
                scenarios = controller.list_scenarios()
                print("Available scenarios:")
                for scenario in scenarios:
                    print(f"  {scenario}")
            
            elif args.resource == "checkpoints":
                checkpoints = controller.list_checkpoints()
                print("Available checkpoints:")
                for ckpt in checkpoints:
                    print(f"  {ckpt['name']} ({ckpt['modified']})")
            
            elif args.resource == "results":
                results_dirs = [d for d in controller.results_dir.iterdir() if d.is_dir()]
                print("Available results directories:")
                for results_dir in sorted(results_dirs, key=lambda x: x.stat().st_mtime, reverse=True):
                    modified = datetime.fromtimestamp(results_dir.stat().st_mtime)
                    print(f"  {results_dir.name} ({modified.strftime('%Y-%m-%d %H:%M:%S')})")
    
    except KeyboardInterrupt:
        print("\\nOperation cancelled by user.")
        sys.exit(1)
    except Exception as e:
        logger.error(f"Command failed: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()